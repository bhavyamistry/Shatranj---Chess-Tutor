{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[2]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "from imblearn.under_sampling import RandomUnderSampler\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.base import BaseEstimator, TransformerMixin\n", "from sklearn.impute import SimpleImputer\n", "from sklearn.preprocessing import StandardScaler,OneHotEncoder, LabelEncoder, MinMaxScaler\n", "from sklearn.pipeline import Pipeline, FeatureUnion\n", "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_auc_score\n", "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "import catboost as cb\n", "from xgboost import XGBClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.linear_model import LogisticRegression\n", "import time\n", "from sklearn.svm import SVC\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.kernel_approximation import RBFSampler\n", "import tensorflow as tf\n", "from sklearn.compose import ColumnTransformer\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chess_data_df = pd.read_csv(\"../input/chess-games/chess_games.csv\")\n", "print(chess_data_df.head())"]}, {"cell_type": "markdown", "metadata": {}, "source": [""]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Correlation Matrix"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[58]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["calculate correlation matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["corr_matrix = chess_data_df.corr()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["plot correlation matrix using a heatmap"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.heatmap(corr_matrix, annot=True, cmap='rocket')\n", "plt.title('Correlation Matrix')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[59]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["According to the correlation Matrix columns 'WhiteRatingDiff' and  'BlackRatingDiff' <br>\n", "have least correlation and the values just the meant the what difference in rank affects the game<br>\n", "If wins difference is positive or else difference is negative"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[60]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(8, 7))\n", "custom_palette = [\"#8B0000\", \"#FF8C00\", \"#FFD700\", \"#228B22\", \"#00BFFF\", \"#1E90FF\"]\n", "sns.countplot(y='Event', data=chess_data_df, palette=custom_palette, order=chess_data_df['Event'].value_counts().index)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["set x-axis label and scale"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.xlabel('Count')\n", "plt.ticklabel_format(axis='x', style='plain', useOffset=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["set x-axis limits and tick values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.xlim(0, 3000000)\n", "plt.xticks([0, 500000, 1000000, 1500000, 2000000, 2500000,3000000], \n", "           ['0', '0.5M', '1M', '1.5M', '2M', '2.5M','3M'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["display plot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[61]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["create a new dataframe with count of each event"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["event_counts = chess_data_df['Event'].value_counts().reset_index()\n", "event_counts.columns = ['Event', 'Count']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["create a pie chart"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(10,10))\n", "ax.pie(event_counts['Count'], labels=event_counts['Event'], autopct='%1.1f%%', startangle=90, counterclock=False)\n", "ax.set_title('Event Distribution')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[62]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 5))\n", "custom_palette = [\"#8B0000\", \"#FF8C00\", \"#FFD700\", \"#228B22\", \"#00BFFF\", \"#1E90FF\"]\n", "ax = sns.countplot(x='Termination', data=chess_data_df, palette=custom_palette)\n", "for p in ax.patches:\n", "    height = p.get_height()\n", "    ax.text(p.get_x() + p.get_width()/2., height + 3, str(round(height/1000000,2))+'M', ha='center')\n", "# set y-axis label and scale\n", "# set y-axis label and scale\n", "plt.ylabel('Count')\n", "plt.ticklabel_format(axis='y', style='plain', useOffset=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["set y-axis limits and tick values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.ylim(0, 6000000)\n", "plt.yticks([0, 1000000, 2000000, 3000000, 4000000, 5000000], \n", "           ['0', '1M', '2M', '3M', '4M', '5M'])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[63]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 5))\n", "custom_palette = [\"#8B0000\", \"#FF8C00\", \"#FFD700\", \"#228B22\", \"#00BFFF\", \"#1E90FF\"]\n", "ax = sns.countplot(x='Result', data=chess_data_df, palette=custom_palette)\n", "for p in ax.patches:\n", "    height = p.get_height()\n", "    ax.text(p.get_x() + p.get_width()/2., height + 3, str(round(height/1000000,2))+'M', ha='center')\n", "# set y-axis label and scale\n", "# set y-axis label and scale\n", "plt.ylabel('Count')\n", "plt.ticklabel_format(axis='y', style='plain', useOffset=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["set y-axis limits and tick values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.ylim(0, 6000000)\n", "plt.yticks([0, 1000000, 2000000, 3000000, 4000000, 5000000], \n", "           ['0', '1M', '2M', '3M', '4M', '5M'])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[64]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up the figure size and style"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.set(rc={'figure.figsize':(10,5)})\n", "sns.set_style(\"whitegrid\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a list of the two columns to plot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = [chess_data_df['WhiteElo'], chess_data_df['BlackElo']]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the box plot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bp = sns.boxplot(data=data, palette='rocket', showfliers=True, width=0.5)\n", "bp.set(xlabel='User Ratings', xticklabels=['WhiteElo', 'BlackElo'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Show the plot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[65]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.histplot(chess_data_df['WhiteElo'],palette='rocket')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[66]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.histplot(chess_data_df['BlackElo'])"]}, {"cell_type": "markdown", "metadata": {}, "source": [""]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Preprocessing"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[88]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_chess_data_df = chess_data_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[89]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Row-wise missing value analysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["missing_values_row = new_chess_data_df.isnull().sum(axis=1)\n", "print(\"Number of rows with missing values:\", len(missing_values_row[missing_values_row > 0]))\n", "print(\"Percentage of rows with missing values:\", round(len(missing_values_row[missing_values_row > 0]) / len(chess_data_df) * 100, 2), \"%\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Column-wise missing value analysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["missing_values_column = new_chess_data_df.isnull().sum()\n", "print(\"Number of columns with missing values:\", len(missing_values_column[missing_values_column > 0]))\n", "print(\"Percentage of columns with missing values:\", round(len(missing_values_column[missing_values_column > 0]) / len(chess_data_df.columns) * 100, 2), \"%\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Overall missing value analysis"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["total_missing_values = new_chess_data_df.isnull().sum().sum()\n", "print(\"Total number of missing values:\", total_missing_values)\n", "print(\"Percentage of missing values:\", round(total_missing_values / (len(chess_data_df) * len(chess_data_df.columns)) * 100, 2), \"%\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[90]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Drop all rows with NaNs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_chess_data_df.dropna(inplace=True)\n", "# Print the shape of the cleaned dataframe\n", "print(\"Shape of dataframe after dropping NaNs:\", new_chess_data_df.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[91]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Drop the columns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_chess_data_df.drop(['White', 'Black', 'UTCDate', 'UTCTime', 'WhiteRatingDiff', 'BlackRatingDiff', 'TimeControl'], axis=1, inplace=True)\n", "# Print the updated dataframe\n", "# print(chess_data_df.head())\n", "print(new_chess_data_df.shape)\n", "# filter the rows based on the condition\n", "new_chess_data_df = new_chess_data_df[(new_chess_data_df['AN'].str.len() >= 40) & (~new_chess_data_df['AN'].str.contains('{'))]\n", "# reset the index after dropping the rows\n", "new_chess_data_df.reset_index(drop=True, inplace=True)\n", "print(new_chess_data_df.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[92]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(new_chess_data_df.columns)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[93]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["an_val = new_chess_data_df['AN']\n", "# initialize dataframe with empty values\n", "#print(an_val[0])\n", "# loop through each game and extract the first 3 moves\n", "dflist = []\n", "for i, game in enumerate(an_val):\n", "    itr = 0\n", "    temp = []\n", "    flag = 1\n", "    tstr = ''\n", "    if(len(game)<40 or '{' in game):\n", "        continue\n", "    #print(len(game), game)\n", "    while(flag and itr<len(game)):\n", "        if game[itr] == \" \":\n", "            temp.append(tstr)\n", "            tstr = ''\n", "        else:\n", "            tstr += game[itr]\n", "        itr+=1\n", "        if(itr+1<len(game)):\n", "            if(game[itr]=='4' and game[itr+1]=='.'):\n", "                flag = 0\n", "    #print(temp)\n", "    temp1 = []\n", "    for i in range(0,len(temp)):\n", "        if i!=0 and i!=3 and i!=6:\n", "            temp1.append(temp[i])\n", "    dflist.append(temp1)\n", "df = pd.DataFrame(dflist, columns=['w1', 'b1', 'w2', 'b2', 'w3', 'b3'])\n", "print(df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[94]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["merge the two dataframes on their indices"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["merged_df = pd.merge(new_chess_data_df, df, left_index=True, right_index=True)\n", "print(merged_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[95]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_df = merged_df\n", "new_df.drop(['Event', 'AN', 'Opening', 'Termination', 'ECO'], axis=1, inplace=True)\n", "new_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[96]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["create a mapping dictionary to replace the values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mapping_dict = {'1-0': 1, '0-1': 0, '1/2-1/2': 2}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["replace the values in the 'Result' column using the mapping dictionary"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_df['Result'] = new_df['Result'].replace(mapping_dict)\n", "new_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[97]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_df['Result'].unique()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[98]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_df = new_df.loc[new_df['Result'] != 2]\n", "new_df = new_df.loc[new_df['Result'] != '*']\n", "print(new_df)\n", "new_df['Result'].unique()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[99]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "import pandas as pd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a new dataframe with only the features and target variable"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = new_df.drop(\"Result\", axis=1)\n", "y = new_df[\"Result\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Use train_test_split to get a stratified sample of 50000 rows"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X, _, y, _ = train_test_split(X, y, train_size=50000, stratify=y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print the value counts of the target variable in the stratified sample"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(y.value_counts())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[100]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(X)\n", "print(y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[101]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_df = pd.concat([X, y], axis=1)\n", "new_df = new_df.reset_index(drop = True)\n", "new_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[102]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import OneHotEncoder\n", "# create an instance of the OneHotEncoder\n", "onehot_encoder = OneHotEncoder()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["fit the encoder on the columns 'w1', 'b1', 'w2', 'b2', 'w3', 'b3'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["onehot_encoder.fit(new_df[['w1', 'b1', 'w2', 'b2', 'w3', 'b3']].values)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["transform the columns using one-hot encoding"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["onehot_encoded = onehot_encoder.transform(new_df[['w1', 'b1', 'w2', 'b2', 'w3', 'b3']].values)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["create a new dataframe with the one-hot encoded columns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["onehot_df = pd.DataFrame(onehot_encoded.toarray(), columns=onehot_encoder.get_feature_names_out(['w1', 'b1', 'w2', 'b2', 'w3', 'b3']))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["concatenate the new dataframe with the original dataframe"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_df = pd.concat([new_df, onehot_df], axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["drop the original columns 'w1', 'b1', 'w2', 'b2', 'w3', 'b3'"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_df = new_df.drop(['w1', 'b1', 'w2', 'b2', 'w3', 'b3'], axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["print the modified dataframe"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(new_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[103]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_df = new_df.dropna()\n", "print(new_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[104]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = new_df\n", "Y = new_df['Result']\n", "X.drop(['Result'], axis = 1, inplace = True)\n", "print(X)\n", "print(Y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[84]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, stratify = Y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[85]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.naive_bayes import GaussianNB\n", "GNBclf = GaussianNB()\n", "model = GNBclf.fit(X_train, Y_train)\n", "preds = GNBclf.predict(X_test)\n", "print(preds)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[109]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import classification_report\n", "print(classification_report(preds, Y))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[80]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ytest = Y_test.values\n", "ytest = ytest.astype(int)\n", "ytest"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[82]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["preds = preds.astype(int)\n", "preds"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[83]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracy = accuracy_score(ytest, preds)\n", "print(\"Accuracy:\", accuracy)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Filter the rows with Termination value Abandoned or Rule Infraction and drop them"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_chess_data_df = new_chess_data_df[~new_chess_data_df['Termination'].isin(['Abandoned', 'Rule Infraction'])]\n", "# Print the shape of the updated dataframe\n", "print(\"Shape of the updated dataframe:\", new_chess_data_df.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[17]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(new_chess_data_df.describe())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[18]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Number of rows with '*' in column 'Result': \", (new_chess_data_df['Result'] == '*').sum())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[19]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_chess_data_df.head(5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[20]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eco_mapping = new_chess_data_df[['ECO', 'Opening']].drop_duplicates(subset='ECO')\n", "eco_mapping"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[21]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["eco_mapping.to_csv('eco_opening_mapping.csv', index=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Print a message to indicate that the dataframe has been saved to a .csv file"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Dataframe has been saved to 'eco_opening_mapping.csv'\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[22]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_chess_data_df.drop(['Opening'], axis=1, inplace=True)\n", "new_chess_data_df.head(5)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[23]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for col in new_chess_data_df.columns:\n", "    if(len(new_chess_data_df[col].unique())<=20):\n", "        print(\"Dealing with:\", col)\n", "        print(new_chess_data_df[col].unique())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[25]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dealing with ECO values<br>\n", "considering top 25 percentile White and Black ELO rated players"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["high_rated_whites = new_chess_data_df[new_chess_data_df[\"WhiteElo\"] >= 1919]\n", "high_rated_blacks = new_chess_data_df[new_chess_data_df[\"BlackElo\"] >= 1919]\n", "print(\"Total number of rows with top 25 percentile White and Black ELO rated players: \", (high_rated_whites.shape[0]+high_rated_blacks.shape[0]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dealing with ECO values<br>\n", "considering bottom 25 percentile White and Black ELO rated players"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["low_rated_whites = new_chess_data_df[new_chess_data_df[\"WhiteElo\"] < 1559]\n", "low_rated_blacks = new_chess_data_df[new_chess_data_df[\"BlackElo\"] < 1557]\n", "print(\"Total number of rows with bottom 25 percentile White and Black ELO rated players: \", (low_rated_whites.shape[0]+low_rated_blacks.shape[0]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dealing with ECO values<br>\n", "considering average rated White and Black ELO players"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["avg_rated_whites = new_chess_data_df[(new_chess_data_df[\"WhiteElo\"] >= 1559) & (new_chess_data_df[\"WhiteElo\"] < 1919)]\n", "avg_rated_blacks = new_chess_data_df[(new_chess_data_df[\"BlackElo\"] >= 1557) & (new_chess_data_df[\"BlackElo\"] < 1919)]\n", "print(\"Total number of rows with average White and Black ELO rated players: \", (avg_rated_whites.shape[0]+avg_rated_blacks.shape[0]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[26]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(new_chess_data_df)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[28]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "os.chdir(r'/kaggle/working')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[29]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_chess_data_df.to_csv(r'new_chess_data_df.csv')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[8]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chess_data_df = pd.read_csv(\"chess_games.csv\")\n", "chess_data_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[9]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chess_data_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[10]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chess_data_df.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[3]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chess_data_df_prep = pd.read_csv(\"/kaggle/input/dataset/chessdbpp.csv\")\n", "chess_data_df_prep.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[4]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["chess_data_df_prep.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[5]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = chess_data_df_prep.drop(\"Result\", axis=1)\n", "y = chess_data_df_prep[\"Result\"]\n", "cat_attributes = ['w1','b1','w2','b2','w3','b3']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[6]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_Train, X_Test, Y_Train, Y_Test = train_test_split(X,y, train_size=100000, stratify=y,test_size=20000, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["print(X_Train.shape, X_Test.shape)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_Train, X_Valid, Y_Train, Y_Valid = train_test_split(X,y, train_size=100000, stratify=y,test_size=10000, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[7]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_Train.shape, X_Test.shape, X_Valid.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[8]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["define undersample strategy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["random_undersampler = RandomUnderSampler(sampling_strategy='majority')\n", "# fit and apply the transform\n", "X_under, Y_under = random_undersampler.fit_resample(X_Train, Y_Train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[9]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_under.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[11]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_under.shape, Y_under.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[15]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_attributes = ['WhiteElo','BlackElo']\n", "cat_attributes = ['w1','b1','w2','b2','w3','b3']\n", "print(num_attributes)\n", "print(cat_attributes)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[13]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class DataFrameSelector(BaseEstimator, TransformerMixin):\n", "    def __init__(self, attribute_names):\n", "        self.attribute_names = attribute_names\n", "    \n", "    def fit(self, X, y=None):\n", "        return self\n", "    \n", "    def transform(self, X):\n", "        return X[self.attribute_names].values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[16]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_pipeline = Pipeline([\n", "        ('selector', DataFrameSelector(num_attributes)),\n", "        ('std_scaler', StandardScaler()),\n", "    ])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cat_pipeline = Pipeline([\n", "        ('selector', DataFrameSelector(cat_attributes)),\n", "        # ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n", "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n", "    ])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_prep_pipeline = FeatureUnion(transformer_list=[\n", "        (\"num_pipeline\", num_pipeline),\n", "        (\"cat_pipeline\", cat_pipeline),\n", "    ])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[210]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_Test.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[201]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["le = LabelEncoder()<br>\n", "T_train = le.fit_transform(X_under)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["create an instance of the decision tree classifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf = DecisionTreeClassifier()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["fit the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clf.fit(X_Train, Y_Train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["make predictions on the test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = clf.predict(X_Test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[17]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["expLog = pd.DataFrame()\n", "del expLog\n", "try:\n", "    expLog\n", "except NameError:\n", "    expLog = pd.DataFrame(columns=[ \n", "                                   \"Model name\",\n", "                                   \"Train Acc\", \n", "                                   \"Valid Acc\",\n", "                                   \"Test  Acc\",\n", "                                   \"Train AUC\", \n", "                                   \"Valid AUC\",\n", "                                   \"Test  AUC\",\n", "                                   \"Train F1\", \n", "                                   \"Valid F1\",\n", "                                   \"Test F1\",\n", "                                   \"Precision\",\n", "                                   \"Recall\",\n", "                                   \"Fit Time (seconds)\"\n", "                                  ])\n", "expLog"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[18]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(42)\n", "def run_model(model, X_train, Y_train, X_test, Y_test, X_valid, Y_valid):\n", "    start_time = time.time()\n", "    full_pipeline_with_predictor = Pipeline([\n", "            (\"preparation\", data_prep_pipeline),\n", "#             (\"pca\", PCA(n_components=60)),\n", "            (\"model\", model)\n", "        ])\n", "    model = full_pipeline_with_predictor.fit(X_train, Y_train)\n", "    model_name = \"{}\".format(type(full_pipeline_with_predictor['model']).__name__)\n", "    fit_time = time.time() - start_time\n", "    expLog.loc[len(expLog)] = [model_name] + list(np.round(\n", "                   [accuracy_score(Y_train, model.predict(X_train)), \n", "                    accuracy_score(Y_valid, model.predict(X_valid)),\n", "                    accuracy_score(Y_test, model.predict(X_test)),\n", "                    roc_auc_score(Y_train, model.predict_proba(X_train)[:, 1]),\n", "                    roc_auc_score(Y_valid, model.predict_proba(X_valid)[:, 1]),\n", "                    roc_auc_score(Y_test, model.predict_proba(X_test)[:, 1]),\n", "                    f1_score(Y_train, model.predict(X_train)), \n", "                    f1_score(Y_valid, model.predict(X_valid)),\n", "                    f1_score(Y_test, model.predict(X_test)),\n", "                    precision_score(Y_test, model.predict(X_test)),\n", "                    recall_score(Y_test, model.predict(X_test)),\n", "                    fit_time], 4))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[19]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["clfs = [cb.CatBoostClassifier(),\n", "        DecisionTreeClassifier(),\n", "        GaussianNB(),RandomForestClassifier(),GradientBoostingClassifier(),XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),AdaBoostClassifier(random_state=42)]\n", "for cl in clfs:\n", "    print(cl)\n", "    run_model(cl, X_Train, Y_Train, X_Test, Y_Test, X_Valid, Y_Valid)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[20]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["expLog"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[28]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_model_SVC(model, X_train, Y_train, X_test, Y_test, X_valid, Y_valid):\n", "    start_time = time.time()\n", "    full_pipeline_with_predictor = Pipeline([\n", "            (\"preparation\", data_prep_pipeline),\n", "            RBFSampler(gamma=1, random_state=42),\n", "            (\"model\", model)\n", "        ])\n", "    model = full_pipeline_with_predictor.fit(X_train, Y_train)\n", "    model_name = \"{}\".format(type(full_pipeline_with_predictor['model']).__name__)\n", "    fit_time = time.time() - start_time\n", "    expLog.loc[len(expLog)] = [model_name] + list(np.round(\n", "                   [accuracy_score(Y_train, model.predict(X_train)), \n", "                    accuracy_score(Y_valid, model.predict(X_valid)),\n", "                    accuracy_score(Y_test, model.predict(X_test)),\n", "                    roc_auc_score(Y_train, model.predict_proba(X_train)[:, 1]),\n", "                    roc_auc_score(Y_valid, model.predict_proba(X_valid)[:, 1]),\n", "                    roc_auc_score(Y_test, model.predict_proba(X_test)[:, 1]),\n", "                    f1_score(Y_train, model.predict(X_train)), \n", "                    f1_score(Y_valid, model.predict(X_valid)),\n", "                    f1_score(Y_test, model.predict(X_test)),\n", "                    precision_score(Y_test, model.predict(X_test)),\n", "                    recall_score(Y_test, model.predict(X_test)),\n", "                    fit_time], 4))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[28]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(num_attributes)\n", "print(cat_attributes)\n", "# Create a StandardScaler object\n", "scaler = StandardScaler()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a OneHotEncoder object"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["encoder = OneHotEncoder(handle_unknown=\"ignore\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit the StandardScaler object to the numerical data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_Train_fitted = scaler.fit(X_Train[num_attributes])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Transform the numerical data using the StandardScaler object"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train_scaled = scaler.transform(X_Train_fitted)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit the OneHotEncoder object to the categorical data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train_categorical = encoder.fit(X_Train[cat_attributes])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Transform the categorical data using the OneHotEncoder object"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train_categorical_encoded = encoder.transform(X_train_categorical)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Concatenate the scaled numerical data and the encoded categorical data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_Train = np.concatenate([X_train_scaled, X_train_categorical_encoded], axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit the StandardScaler object to the numerical data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_Test_fitted = scaler.fit(X_Test[num_attributes])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Transform the numerical data using the StandardScaler object"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_test_scaled = scaler.transform(X_Test_fitted)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit the OneHotEncoder object to the categorical data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_test_categorical = encoder.fit(X_Train[cat_attributes])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Transform the categorical data using the OneHotEncoder object"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_test_categorical_encoded = encoder.transform(X_test_categorical)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Concatenate the scaled numerical data and the encoded categorical data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_Test = np.concatenate([X_test_scaled, X_test_categorical_encoded], axis=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[26]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if tf.test.is_gpu_available():\n", "    # Use GPU\n", "    device = \"/gpu:0\"\n", "else:\n", "    # Use CPU\n", "    device = \"/cpu:0\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(Dense(64, input_dim=X_Train.shape[1], activation='relu'))\n", "model.add(Dense(32, activation='relu'))\n", "model.add(Dense(1, activation='sigmoid'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compile the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.fit(X_Train, Y_Train, epochs=100, batch_size=32, validation_data=(X_Valid, Y_Valid))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Predict the classes for the test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = model.predict_classes(X_Test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate the performance using accuracy, F1, precision, and recall metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracy = accuracy_score(Y_Test, y_pred)\n", "f1 = f1_score(Y_Test, y_pred)\n", "precision = precision_score(Y_Test, y_pred)\n", "recall = recall_score(Y_Test, y_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Accuracy: {:.2f}\".format(accuracy))\n", "print(\"F1 score: {:.2f}\".format(f1))\n", "print(\"Precision: {:.2f}\".format(precision))\n", "print(\"Recall: {:.2f}\".format(recall))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[44]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["expLog"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.fit(X_train, Y_train)\n", "# save the model to disk\n", "filename = 'finalized_model.sav'\n", "pickle.dump(model, open(filename, 'wb'))\n", " \n", "# some time later...\n", " \n", "# load the model from disk\n", "loaded_model = pickle.load(open(filename, 'rb'))\n", "result = loaded_model.score(X_test, Y_test)\n", "print(result)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}